#!/usr/bin/env python3
"""
åœ°åŸŸãƒ»ä¾¡æ ¼ãƒ»ç‰¹å¾´ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
"""
from scraper.yuryoweb_scraper import YuryoWebScraper
import json
import time
from datetime import datetime

def fetch_area_price_feature():
    """åœ°åŸŸãƒ»ä¾¡æ ¼ãƒ»ç‰¹å¾´ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—"""
    
    print("="*80)
    print("åœ°åŸŸãƒ»ä¾¡æ ¼ãƒ»ç‰¹å¾´ã‚«ãƒ†ã‚´ãƒªã®å–å¾—")
    print(f"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–
    scraper = YuryoWebScraper(delay_seconds=1.5)
    
    # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±å–å¾—
    print("\nã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’å–å¾—ä¸­...")
    categories = scraper.fetch_categories()
    
    all_data = []
    
    # åœ°åŸŸã€ä¾¡æ ¼ã€ç‰¹å¾´ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    for group_name in ['area', 'price', 'feature']:
        category_list = categories.get(group_name, [])
        
        print(f"\n{'='*60}")
        print(f"{group_name.upper()}ã‚°ãƒ«ãƒ¼ãƒ—: {len(category_list)}ã‚«ãƒ†ã‚´ãƒª")
        print(f"{'='*60}")
        
        group_data = []
        
        for idx, category in enumerate(category_list, 1):
            print(f"\n[{idx}/{len(category_list)}] {category['name']}")
            print(f"  URL: {category['url']}")
            
            count = 0
            category_data = []
            
            try:
                start_time = time.time()
                
                for data in scraper.scrape_category(
                    category_url=category['url'],
                    limit=None,  # å…¨ä»¶å–å¾—
                    category_group=group_name,
                    category_name=category['name']
                ):
                    category_data.append(data)
                    all_data.append(data)
                    group_data.append(data)
                    count += 1
                    
                    if count % 100 == 0:
                        print(f"    {count}ä»¶å–å¾—æ¸ˆã¿...", end='\r')
                
                elapsed = time.time() - start_time
                print(f"  âœ… {count}ä»¶å–å¾—å®Œäº† ({elapsed:.1f}ç§’)")
                
                # 5ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ä¸­é–“ä¿å­˜
                if idx % 5 == 0:
                    temp_file = f"{group_name}_temp_{idx}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                    with open(temp_file, 'w', encoding='utf-8') as f:
                        json.dump(group_data, f, ensure_ascii=False, indent=2)
                    print(f"  ğŸ’¾ ä¸­é–“ä¿å­˜: {temp_file} ({len(group_data)}ä»¶)")
                
            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # ã‚°ãƒ«ãƒ¼ãƒ—å…¨ä½“ã‚’ä¿å­˜
        group_file = f"{group_name}_complete_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(group_file, 'w', encoding='utf-8') as f:
            json.dump(group_data, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… {group_name.upper()}ã‚°ãƒ«ãƒ¼ãƒ—ä¿å­˜: {group_file}")
        print(f"   å–å¾—ä»¶æ•°: {len(group_data)}ä»¶")
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    output_file = f"area_price_feature_all_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'='*80}")
    print(f"âœ… å…¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")
    print(f"   ãƒ•ã‚¡ã‚¤ãƒ«å: {output_file}")
    print(f"   ç·ä»¶æ•°: {len(all_data)}ä»¶")
    print(f"å®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*80}")
    
    return len(all_data)

if __name__ == "__main__":
    count = fetch_area_price_feature()
    print(f"\næœ€çµ‚å–å¾—ä»¶æ•°: {count:,}ä»¶")