#!/usr/bin/env python3
"""
å…¨ä¼æ¥­ã®å•ã„åˆã‚ã›URLæŠ½å‡ºã—ã¦æœ€çµ‚CSVã«è¿½åŠ 
"""
import csv
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
from datetime import datetime
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# å•ã„åˆã‚ã›é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
CONTACT_KEYWORDS = [
    'contact', 'inquiry', 'form', 'å•ã„åˆã‚ã›', 'ãŠå•ã„åˆã‚ã›', 'å•åˆã›', 'å•åˆã‚ã›',
    'toiawase', 'otoiawase', 'mail', 'ãƒ¡ãƒ¼ãƒ«', 'ãƒ•ã‚©ãƒ¼ãƒ ', 'ç›¸è«‡', 'soudan',
    'è¦‹ç©', 'estimate', 'mitsumori', 'è³‡æ–™è«‹æ±‚', 'request', 'ç”³ã—è¾¼ã¿', 'apply',
    'support', 'ã‚µãƒãƒ¼ãƒˆ', 'ã”ç›¸è«‡', 'ã”è³ªå•', 'question'
]

# é™¤å¤–ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
EXCLUDE_KEYWORDS = [
    'privacy', 'policy', 'terms', 'ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼', 'è¦ç´„', 'åˆ©ç”¨è¦ç´„',
    'sitemap', 'ã‚µã‚¤ãƒˆãƒãƒƒãƒ—', 'recruit', 'æ¡ç”¨', 'career', 'login', 'ãƒ­ã‚°ã‚¤ãƒ³'
]

class ContactExtractor:
    def __init__(self, timeout=7, max_workers=15):
        self.timeout = timeout
        self.max_workers = max_workers
        self.lock = threading.Lock()
        self.results = {}
        self.processed = 0
        self.success = 0
        self.found = 0
        self.start_time = time.time()
        
    def create_session(self):
        """æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        return session
        
    def is_contact_url(self, url, text=''):
        """URLã‚„ä»˜éšãƒ†ã‚­ã‚¹ãƒˆãŒå•ã„åˆã‚ã›é–¢é€£ã‹åˆ¤å®š"""
        url_lower = url.lower()
        text_lower = text.lower()
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        for keyword in EXCLUDE_KEYWORDS:
            if keyword in url_lower or keyword in text_lower:
                return False
        
        # å•ã„åˆã‚ã›ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        for keyword in CONTACT_KEYWORDS:
            if keyword in url_lower or keyword in text_lower:
                return True
        
        return False
    
    def extract_from_site(self, company):
        """1ã¤ã®ã‚µã‚¤ãƒˆã‹ã‚‰å•ã„åˆã‚ã›URLã‚’æŠ½å‡º"""
        session = self.create_session()
        site_url = company['official_site_url']
        
        try:
            # URLã®æ­£è¦åŒ–
            if not site_url.startswith(('http://', 'https://')):
                site_url = 'https://' + site_url
            
            # ã‚µã‚¤ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹
            response = session.get(site_url, timeout=self.timeout, allow_redirects=True)
            response.raise_for_status()
            
            # HTMLã‚’ãƒ‘ãƒ¼ã‚¹
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # å•ã„åˆã‚ã›é–¢é€£ã®ãƒªãƒ³ã‚¯ã‚’åé›†
            contact_urls = set()
            
            # å…¨ã¦ã®ãƒªãƒ³ã‚¯ã‚’ç¢ºèª
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                if not href or href.startswith('#'):
                    continue
                    
                text = link.get_text(strip=True)
                
                # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                full_url = urljoin(response.url, href)
                
                # åŒã˜ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ã®ã¿
                if urlparse(full_url).netloc == urlparse(response.url).netloc:
                    if self.is_contact_url(full_url, text):
                        contact_urls.add(full_url)
                        # æœ€åˆã®1ã¤ã ã‘ã§ååˆ†
                        if contact_urls:
                            break
            
            # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸è‡ªä½“ãŒå•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ ã®å ´åˆã‚’ãƒã‚§ãƒƒã‚¯
            if not contact_urls:
                page_text = soup.get_text().lower()
                if any(keyword in page_text for keyword in ['ãŠåå‰', 'ä¼šç¤¾å', 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'ãŠå•ã„åˆã‚ã›å†…å®¹', 'name', 'email', 'message']):
                    # ãƒ•ã‚©ãƒ¼ãƒ è¦ç´ ãŒã‚ã‚‹ã‹ç¢ºèª
                    if soup.find('form'):
                        contact_urls.add(response.url)
            
            # çµæœã‚’ä¿å­˜
            contact_url = list(contact_urls)[0] if contact_urls else ''
            
            with self.lock:
                self.results[company['official_site_url']] = contact_url
                self.processed += 1
                self.success += 1
                if contact_url:
                    self.found += 1
                
                # é€²æ—è¡¨ç¤ºï¼ˆ100ä»¶ã”ã¨ï¼‰
                if self.processed % 100 == 0:
                    elapsed = time.time() - self.start_time
                    rate = self.processed / elapsed
                    remaining = (10021 - self.processed) / rate
                    print(f"  é€²æ—: {self.processed:,}/10,021ä»¶ | æˆåŠŸ: {self.success:,} | ç™ºè¦‹: {self.found:,} | æ®‹ã‚Šæ™‚é–“: {remaining/60:.1f}åˆ†")
            
            return contact_url
            
        except:
            with self.lock:
                self.results[company['official_site_url']] = ''
                self.processed += 1
                
                if self.processed % 100 == 0:
                    elapsed = time.time() - self.start_time
                    rate = self.processed / elapsed
                    remaining = (10021 - self.processed) / rate
                    print(f"  é€²æ—: {self.processed:,}/10,021ä»¶ | æˆåŠŸ: {self.success:,} | ç™ºè¦‹: {self.found:,} | æ®‹ã‚Šæ™‚é–“: {remaining/60:.1f}åˆ†")
            
            return ''
        finally:
            session.close()
    
    def process_all_companies(self, companies):
        """å…¨ä¼æ¥­ã‚’ä¸¦åˆ—å‡¦ç†"""
        
        total = len(companies)
        print(f"å‡¦ç†é–‹å§‹: {total:,}ç¤¾")
        print(f"ä¸¦åˆ—æ•°: {self.max_workers}")
        print(f"æ¨å®šæ™‚é–“: 20-30åˆ†")
        print("-" * 60)
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self.extract_from_site, company): company
                for company in companies
            }
            
            for future in as_completed(futures):
                future.result()
        
        return self.results

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    
    print("="*80)
    print("å•ã„åˆã‚ã›URLæŠ½å‡º & æœ€çµ‚CSVä½œæˆ")
    print(f"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)
    
    # æ—¢å­˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    input_csv = 'yuryoweb_final_complete_20250814_001100.csv'
    companies = []
    company_data = {}
    
    print(f"\nä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {input_csv}")
    
    try:
        with open(input_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                companies.append({
                    'company_name': row['company_name'],
                    'official_site_url': row['official_site_url']
                })
                company_data[row['official_site_url']] = row
        
        print(f"  âœ… {len(companies):,}ç¤¾ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # å‡¦ç†å®Ÿè¡Œ
    print(f"\nå•ã„åˆã‚ã›URLæŠ½å‡ºã‚’é–‹å§‹ã—ã¾ã™...")
    start_time = time.time()
    
    extractor = ContactExtractor(timeout=7, max_workers=15)
    contact_results = extractor.process_all_companies(companies)
    
    elapsed = time.time() - start_time
    
    # çµæœã®é›†è¨ˆ
    found_count = sum(1 for url in contact_results.values() if url)
    
    print(f"\n{'='*60}")
    print("æŠ½å‡ºå®Œäº†:")
    print(f"  å‡¦ç†ä¼æ¥­æ•°: {len(contact_results):,}ç¤¾")
    print(f"  å•ã„åˆã‚ã›URLç™ºè¦‹: {found_count:,}ç¤¾ ({found_count/len(contact_results)*100:.1f}%)")
    print(f"  å‡¦ç†æ™‚é–“: {elapsed:.1f}ç§’ ({elapsed/60:.1f}åˆ†)")
    print(f"{'='*60}")
    
    # æœ€çµ‚CSVã‚’ä½œæˆï¼ˆå•ã„åˆã‚ã›URLä»˜ãï¼‰
    output_file = f"yuryoweb_complete_with_contact_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    
    print(f"\næœ€çµ‚CSVã‚’ä½œæˆä¸­...")
    
    with open(output_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆå•ã„åˆã‚ã›URLåˆ—ã‚’è¿½åŠ ï¼‰
        writer.writerow([
            'company_name',
            'official_site_url',
            'contact_url',  # æ–°è¦è¿½åŠ 
            'yuryoweb_url',
            'address',
            'area_categories',
            'price_categories',
            'feature_categories',
            'industry_categories'
        ])
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã¿
        for url, company_info in company_data.items():
            contact_url = contact_results.get(url, '')
            
            writer.writerow([
                company_info['company_name'],
                company_info['official_site_url'],
                contact_url,  # å•ã„åˆã‚ã›URL
                company_info['yuryoweb_url'],
                company_info['address'],
                company_info['area_categories'],
                company_info['price_categories'],
                company_info['feature_categories'],
                company_info['industry_categories']
            ])
    
    print(f"âœ… æœ€çµ‚CSVä½œæˆå®Œäº†: {output_file}")
    
    # ã‚µãƒãƒªãƒ¼CSVã‚‚ä½œæˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
    summary_file = f"contact_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    
    with open(summary_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['company_name', 'official_site_url', 'contact_url'])
        
        for url, contact_url in contact_results.items():
            company_name = company_data[url]['company_name']
            writer.writerow([company_name, url, contact_url])
    
    print(f"âœ… ã‚µãƒãƒªãƒ¼CSVä½œæˆå®Œäº†: {summary_file}")
    
    # çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ
    stats_file = f"extraction_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(stats_file, 'w', encoding='utf-8') as f:
        f.write("å•ã„åˆã‚ã›URLæŠ½å‡º å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ\n")
        f.write("="*60 + "\n")
        f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"å‡¦ç†ä¼æ¥­æ•°: {len(contact_results):,}ç¤¾\n")
        f.write(f"å•ã„åˆã‚ã›URLç™ºè¦‹: {found_count:,}ç¤¾ ({found_count/len(contact_results)*100:.1f}%)\n")
        f.write(f"å•ã„åˆã‚ã›URLæœªç™ºè¦‹: {len(contact_results)-found_count:,}ç¤¾\n")
        f.write(f"å‡¦ç†æ™‚é–“: {elapsed:.1f}ç§’ ({elapsed/60:.1f}åˆ†)\n")
        f.write(f"å¹³å‡å‡¦ç†é€Ÿåº¦: {len(contact_results)/elapsed:.1f}ç¤¾/ç§’\n")
    
    print(f"âœ… çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {stats_file}")
    
    print(f"\nğŸ‰ å…¨å‡¦ç†å®Œäº†ï¼")
    print(f"å®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)

if __name__ == "__main__":
    main()